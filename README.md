# CrawlProject
Grab douban.com  books data
小结：
由于时间问题，我按照要求实现了其主要功能，抓取了600本书，并将他们按照评分大小排序并去除了评论数少于1000的，最终保留了100本。
首先开启多个线程，提高数据处理的效率，但是遇到了403，频繁访问网站屏蔽IP的问题，于是通过Sleep方法间隔了线程开启的时间。还有就是通过伪装成浏览器的方式来解决。
其实对于单个线程在很短的时间内将所需程序跑完，这个题目更适合用线程池的方式来解决，不用开启太多的Thread，线程池能够继续使用跑完程序的线程，还能灵活的设置属性，正常所需的线程数，最大承载量，所需附加的缓存的线程数。关于监控线程处理时间的问题，我想通过Thread.currentThread().activeCount()>1的方式来记录运行所需时间。
还有关于代码测试的问题，我用Junit做过一些练习，我也是第一次遇到这种爬虫类的题目，还有很多不足的地方，还有很多需要提高和改进的地方来提高程序的效率和稳定性。
